# -*- coding: utf-8 -*-
"""vggish_with_augmented_data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zzzthBSoSRNlybGg0TxhysbYLS4DmpOo
"""

# !pip list
from __future__ import print_function
from __future__ import absolute_import


import tensorflow as tf
#import tensorflow_hub as hub
import numpy as np
import pandas as pd
import os
import datetime
from posixpath import dirname
import pickle
# import keras
# from preprocess_sound import preprocess_sound
# import keras_tuner as kt
import matplotlib.pyplot as plt

import mel_features
import vggish_params as params
import sys

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D, GlobalMaxPooling2D
# from keras.engine.topology import get_source_inputs
from tensorflow.keras import backend as K

from sklearn.metrics import classification_report


SEED = 42
tf.random.set_seed(SEED)

FOLD_0_TRAIN_SAVED_DS_PATH = '/raid/home/labusermoctar/ptsd_dataset/final_data/folds_saved_ds/fold_0/train_ds_b30s'
FOLD_0_TEST_SAVED_DS_PATH = '/raid/home/labusermoctar/ptsd_dataset/final_data/folds_saved_ds/fold_0/test_ds_b30s'

FOLD_1_TRAIN_SAVED_DS_PATH = '/raid/home/labusermoctar/ptsd_dataset/final_data/folds_saved_ds/fold_1/train_ds_b30s'
FOLD_1_TEST_SAVED_DS_PATH = '/raid/home/labusermoctar/ptsd_dataset/final_data/folds_saved_ds/fold_1/test_ds_b30s'

FOLD_2_TRAIN_SAVED_DS_PATH = '/raid/home/labusermoctar/ptsd_dataset/final_data/folds_saved_ds/fold_2/train_ds_b30s'
FOLD_2_TEST_SAVED_DS_PATH = '/raid/home/labusermoctar/ptsd_dataset/final_data/folds_saved_ds/fold_2/test_ds_b30s'

FOLDS = [0, 1, 2]

folds = FOLDS

# VGGish model weigths paths
WEIGHTS_PATH = '/raid/home/labusermoctar/ptsd_audio_detection/vggish_audioset_weights_without_fc2.h5'
WEIGHTS_PATH_TOP = '/raid/home/labusermoctar/ptsd_audio_detection/vggish_audioset_weights.h5'

# Checkpoints
CHECKPOINT_FILEPATH = './final_training/checkpoint'
SAVED_MODELS_PATH = "./final_training/saved_models"

#Epochs
EPOCHS = 150

# FLAG
IS_COLAB = False

AUTO_BALANCE = True

classes = ["Without PTSD", "With PTSD"]

EXAMPLE_WINDOW_SECONDS = 29.76
EXAMPLE_HOP_SECONDS = 29.76
NUM_FRAMES = 2976

BATCH_SIZE = 128

USE_SAVED_DATASET = True

INPUT_SHAPE = (NUM_FRAMES, params.NUM_BANDS, 1)
# INPUT_SHAPE = (2976, 64, 1)

def plot_acc(history, end_offset=0, name=''):
  acc=history.history['accuracy']
  if end_offset:
    acc = acc[:-end_offset]
  plt.figure()
  plt.plot(acc)
  plt.title('Model accuracy')
  plt.ylabel('Accuracy')
  plt.xlabel('Epochs')
  plt.legend(['train', ], loc='upper left')
  plot_name = name if name else f"final_training/model_acc_{datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}.png"
  plt.savefig(plot_name)
  plt.show()


def plot_loss(history, end_offset=0, name=''):
  loss = history.history['loss']
  if end_offset:
    loss = loss[:-end_offset]
  plt.figure()
  plt.plot(loss)
  plt.title('Model loss')
  plt.ylabel('Loss')
  plt.xlabel('Epochs')
  plt.legend(['train'], loc='upper left')
  plot_name = name if name else f"final_training/model_acc_{datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}.png"
  plt.savefig(plot_name)
  plt.show()


"""Loading and building model"""


def get_source_inputs(*args, **kwargs):
  ...


"""VGGish model for Keras. A VGG-like model for audio classification
# Reference
- [CNN Architectures for Large-Scale Audio Classification](ICASSP 2017)
"""

def VGGish(load_weights=True, weights='audioset',
           input_tensor=None, input_shape=None,
           out_dim=None, include_top=True, pooling='avg'):
    '''
    An implementation of the VGGish architecture.
    :param load_weights: if load weights
    :param weights: loads weights pre-trained on a preliminary version of YouTube-8M.
    :param input_tensor: input_layer
    :param input_shape: input data shape
    :param out_dim: output dimension
    :param include_top:whether to include the 3 fully-connected layers at the top of the network.
    :param pooling: pooling type over the non-top network, 'avg' or 'max'
    :return: A Keras model instance.
    '''

    if weights not in {'audioset', None}:
        raise ValueError('The `weights` argument should be either '
                         '`None` (random initialization) or `audioset` '
                         '(pre-training on audioset).')

    if out_dim is None:
        out_dim = params.EMBEDDING_SIZE

    # input shape
    if input_shape is None:
        # input_shape = (params.NUM_FRAMES, params.NUM_BANDS, 1)
        input_shape = (NUM_FRAMES, params.NUM_BANDS, 1)

    if input_tensor is None:
        aud_input = Input(shape=input_shape, name='input_1')
    else:
        if not K.is_keras_tensor(input_tensor):
            aud_input = Input(tensor=input_tensor, shape=input_shape, name='input_1')
        else:
            aud_input = input_tensor



    # Block 1
    x = Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same', name='conv1')(aud_input)
    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='pool1')(x)

    # Block 2
    x = Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same', name='conv2')(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='pool2')(x)

    # Block 3
    x = Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='same', name='conv3/conv3_1')(x)
    x = Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='same', name='conv3/conv3_2')(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='pool3')(x)

    # Block 4
    x = Conv2D(512, (3, 3), strides=(1, 1), activation='relu', padding='same', name='conv4/conv4_1')(x)
    x = Conv2D(512, (3, 3), strides=(1, 1), activation='relu', padding='same', name='conv4/conv4_2')(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='pool4')(x)



    if include_top:
        # FC block
        x = Flatten(name='flatten_')(x)
        x = Dense(4096, activation='relu', name='vggish_fc1/fc1_1')(x)
        x = Dense(4096, activation='relu', name='vggish_fc1/fc1_2')(x)
        x = Dense(out_dim, activation="relu", name='vggish_fc2')(x)
    else:
        if pooling == 'avg':
            x = GlobalAveragePooling2D()(x)
        elif pooling == 'max':
            x = GlobalMaxPooling2D()(x)


    if input_tensor is not None:
        inputs = get_source_inputs(input_tensor)
    else:
        inputs = aud_input
    # Create model.
    model = Model(inputs, x, name='VGGish')


    # load weights
    if load_weights:
        if weights == 'audioset':
            if include_top:
                model.load_weights(WEIGHTS_PATH_TOP)
            else:
                model.load_weights(WEIGHTS_PATH)
        else:
            print("failed to load weights")

    return model


def scheduler2(epoch, lr):
    if epoch < 50:
        return lr
    else:
        # elif epoch%10 == 0:
        return lr * tf.math.exp(-0.1)


def create_model():
    base_model_ = VGGish(load_weights=False, include_top=False, input_shape=INPUT_SHAPE, pooling='avg')
    base_model_.load_weights(WEIGHTS_PATH, by_name=True)
    # base_model_.trainable = False
    # base_model_.summary()
    # base_model_.layers.pop()

    model_ = tf.keras.models.Sequential()
    model_.add(base_model_)
    model_.add(tf.keras.layers.Dense(units=32, activation="relu", kernel_constraint=tf.keras.constraints.MaxNorm(3),
                                     kernel_regularizer=tf.keras.regularizers.L2(0.02)))
    # model_.add(tf.keras.layers.Dropout(0.5))
    model_.add(tf.keras.layers.Dense(units=32, activation="relu", kernel_constraint=tf.keras.constraints.MaxNorm(3),
                                     kernel_regularizer=tf.keras.regularizers.L2(0.02)))
    model_.add(tf.keras.layers.Dropout(0.4))
    model_.add(tf.keras.layers.Dense(units=2, activation='softmax'))
    # model_.build()
    model_.summary()

    return model_


training_histories = []

scores = []

start_time = datetime.datetime.now()

datetime_tag = start_time.strftime("%Y%m%d-%H%M%S")

for fold in folds:
    train_ds_path = f"/raid/home/labusermoctar/ptsd_dataset/final_data/folds_saved_ds/fold_{fold}/train_ds_b30s"
    test_ds_path = f"/raid/home/labusermoctar/ptsd_dataset/final_data/folds_saved_ds/fold_{fold}/test_ds_b30s"

    train_ds = tf.data.experimental.load(train_ds_path).unbatch().batch(BATCH_SIZE)
    test_ds = tf.data.experimental.load(test_ds_path).unbatch().batch(BATCH_SIZE)

    X_train = np.concatenate([x for x, y in train_ds], axis=0)
    print(X_train.shape)
    y_train = np.concatenate([y for x, y in train_ds], axis=0)
    y_train = tf.keras.utils.to_categorical(y_train)
    print(y_train.shape)

    X_test = np.concatenate([x for x, y in test_ds], axis=0)
    print(X_test.shape)
    y_test = np.concatenate([y for x, y in test_ds], axis=0)
    y_test = tf.keras.utils.to_categorical(y_test)
    print(y_test.shape)

    model = create_model()

    adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.00003, epsilon=5e-9, beta_1=0.9, beta_2=0.999)
    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),
                   optimizer=adam_optimizer,
                   metrics=['accuracy', ])
    # early_stop = tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss')

    # reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.000001)

    log_dir = f"final_training/tfb_logs/folds_{fold}/fit/" + datetime_tag
    checkpoint_path = f"final_training/checkpoints/folds_{fold}/best_model_{fold}_" + datetime_tag
    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)
    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler2, verbose=1)
    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(
        checkpoint_path,
        monitor='accuracy',
        verbose=0,
        save_best_only=True,
        save_weights_only=False,
    )
    history = model.fit(X_train, y_train, batch_size=128, epochs=100,
                        callbacks=[tensorboard_callback, model_checkpoint])
    #                    callbacks=[tensorboard_callback, model_checkpoint, lr_scheduler])

    plot_acc(history, name=f"final_training/fold_{fold}_model_acc_{datetime_tag}.png")
    plot_loss(history, name=f"final_training/fold_{fold}_model_loss_{datetime_tag}.png")

    score = model.evaluate(X_test, y_test)

    print("Scores fold", fold, score)

    scores.append(score)
    # eval_accuracies.append(scores[1])
    # eval_losses.append(scores[0])

    print("Fold", fold, "All Scores = ", scores)

    training_histories.append(history.history)

    model.save(f"final_training/saved/fold_{fold}_final_model_30s_{datetime_tag}")

    test_predictions = model.predict(X_test)
    predicted_values = np.argmax(test_predictions, axis=1)
    y_test_ = np.concatenate([y for x, y in test_ds], axis=0)

    print("Classification report of fold ", fold)
    print(classification_report(y_test_, predicted_values, target_names=classes))

    print("-"*10, f"End of training and testing fold {fold}", "-"*10)



print(scores)

for i, sc in enumerate(scores):
    print("Loss fold", i, sc[0])
    print("Accuracy fold", i, sc[1])

with open(f'final_training/folds_hist_{datetime_tag}.pkl', 'wb') as file:
    # A new file will be created
    pickle.dump(training_histories, file)
    print("History file saved")

with open(f'final_training/folds_scores_{datetime_tag}.pkl', 'wb') as file:
    # A new file will be created
    pickle.dump(scores, file)
    print("Scores file saved")


mean_acc = np.mean(np.array([y for _, y in scores]))
mean_loss = np.mean(np.array([x for x, _ in scores]))
print("Average scores: Mean Acc =", mean_acc, " --- Mean Loss =", mean_loss)

std_acc = np.std(np.array([y for _, y in scores]))
std_loss = np.std(np.array([x for x, _ in scores]))
print("STD scores: Mean Acc =", std_acc, " --- Mean Loss =", std_loss)


print("-"*25, "DONE", "-"*25)

time_end = datetime.datetime.now()

print(start_time, time_end, "duration=", (time_end - start_time))
