# -*- coding: utf-8 -*-
"""PTSD Text Classification v2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14DkXytxvFAwdDgJawZOoKoCK7uy8_1v3

# Moctar, need to install transformers first
"""

# !pip install transformers
#
# """# Moctar, unzip"""
#
# !unzip dataset.zip
# !unzip train.zip
# !unzip validation.zip
# !unzip test.zip
# !unzip text_split0.zip
# !unzip text_split1.zip
# !unzip text_split2.zip

"""# Text Train-Val-Test Split
## Do not run
"""

import glob
import os
import random
from sklearn.model_selection import train_test_split
import shutil
import numpy as np
import time

random.seed(42)
np.random.seed(42)

src_folder = "/content/drive/MyDrive/No   Yes PTSD videos"

no_ptsd_files = glob.glob(os.path.join(os.path.join(src_folder, "NO PTSD"), "**", "*.mp4"), recursive=True)
no_ptsd_files = list(filter(lambda path: "copy" not in path.lower(), no_ptsd_files))

ptsd_files = glob.glob(os.path.join(os.path.join(src_folder, "PTSD"), "**", "*.mp4"), recursive=True)
ptsd_files = list(filter(lambda path: "copy" not in path.lower(), ptsd_files))

print(len(no_ptsd_files))
print(len(ptsd_files))

raise SystemExit

no_ptsd_files = random.sample(no_ptsd_files, k=len(ptsd_files))

paths = no_ptsd_files + ptsd_files

assert len(paths) == len(set(paths)), "there are duplicates"

train_paths_ptsd, test_paths_ptsd = train_test_split(ptsd_files, test_size=0.2, random_state=42)
test_paths_ptsd, val_paths_ptsd = train_test_split(test_paths_ptsd, test_size=0.5, random_state=42)

train_paths_no_ptsd, test_paths_no_ptsd = train_test_split(no_ptsd_files, test_size=0.2, random_state=42)
test_paths_no_ptsd, val_paths_no_ptsd = train_test_split(test_paths_no_ptsd, test_size=0.5, random_state=42)

print("len(train_paths_ptsd)", len(train_paths_ptsd))
print("len(train_paths_no_ptsd)", len(train_paths_no_ptsd))

print("len(val_paths_ptsd)", len(val_paths_ptsd))
print("len(val_paths_no_ptsd)", len(val_paths_no_ptsd))

print("len(test_paths_ptsd)", len(test_paths_ptsd))
print("len(test_paths_no_ptsd)", len(test_paths_no_ptsd))

train_paths = train_paths_ptsd + train_paths_no_ptsd
val_paths = val_paths_ptsd + val_paths_no_ptsd
test_paths = test_paths_ptsd + test_paths_no_ptsd

text_paths = glob.glob("NO PTSD/**/*.txt", recursive=True) + glob.glob("PTSD/**/*.txt", recursive=True)
text_paths = list(filter(lambda path: "copy" not in path.lower(), text_paths))

substr_is_in_list = lambda s, l: any([s in p.replace("'", "_").replace("%", "_").replace("&", "_") for p in l])

text_train_paths_ptsd = []
text_train_paths_no_ptsd = []
text_val_paths_ptsd = []
text_val_paths_no_ptsd = []
text_test_paths_ptsd = []
text_test_paths_no_ptsd = []
for text_path in text_paths:
    fname = os.path.basename(text_path).replace(".txt", "")
    if substr_is_in_list(fname, train_paths_ptsd):
        text_train_paths_ptsd.append(text_path)
    elif substr_is_in_list(fname, train_paths_no_ptsd):
        text_train_paths_no_ptsd.append(text_path)
    elif substr_is_in_list(fname, val_paths_ptsd):
        text_val_paths_ptsd.append(text_path)
    elif substr_is_in_list(fname, val_paths_no_ptsd):
        text_val_paths_no_ptsd.append(text_path)
    elif substr_is_in_list(fname, test_paths_ptsd):
        text_test_paths_ptsd.append(text_path)
    elif substr_is_in_list(fname, test_paths_no_ptsd):
        text_test_paths_no_ptsd.append(text_path)

print("len(text_train_paths_ptsd)", len(text_train_paths_ptsd))
print("len(text_train_paths_no_ptsd)", len(text_train_paths_no_ptsd))
print("len(text_val_paths_ptsd)", len(text_val_paths_ptsd))
print("len(text_val_paths_no_ptsd)", len(text_val_paths_no_ptsd))
print("len(text_test_paths_ptsd)", len(text_test_paths_ptsd))
print("len(text_test_paths_no_ptsd)", len(text_test_paths_no_ptsd))

text_train_paths = text_train_paths_ptsd + text_train_paths_no_ptsd

print(sorted(map(lambda p: os.path.basename(p), text_train_paths)))
print()
print(sorted(map(lambda p: os.path.basename(p).replace("'", "_").replace("%", "_").replace("&", "_"), train_paths)))

# import shutil

# os.makedirs("train/PTSD")
# os.makedirs("train/NO PTSD")
# os.makedirs("validation/PTSD")
# os.makedirs("validation/NO PTSD")
# os.makedirs("test/PTSD")
# os.makedirs("test/NO PTSD")

# copy_to_dir = lambda dirname, l: [shutil.copyfile(path, os.path.join(dirname, os.path.basename(path))) for path in l]

# copy_to_dir("train/PTSD", text_train_paths_ptsd)
# copy_to_dir("train/NO PTSD", text_train_paths_no_ptsd)
# copy_to_dir("validation/PTSD", text_val_paths_ptsd)
# copy_to_dir("validation/NO PTSD", text_val_paths_no_ptsd)
# copy_to_dir("test/PTSD", text_test_paths_ptsd)
# copy_to_dir("test/NO PTSD", text_test_paths_no_ptsd)

"""# K-Fold Split
## Do not run
"""

from sklearn.model_selection import KFold
import numpy as np

random.seed(42)
np.random.seed(42)


substr_is_in_list = lambda s, l: any([s in p.replace("'", "_").replace("%", "_").replace("&", "_").replace('"', "_") for p in l])
create_dir_if_not_exists = lambda d: os.makedirs(d) if not os.path.isdir(d) else None

def write_list_to_file(l, fn):
    with open(fn, "w") as f:
        for i in l:
            f.write(i)
            f.write("\n")

src_folder = "/content/drive/MyDrive/No   Yes PTSD videos"

no_ptsd_files = glob.glob(os.path.join(os.path.join(src_folder, "NO PTSD"), "**", "*.mp4"), recursive=True)
no_ptsd_files = list(filter(lambda path: "copy" not in path.lower(), no_ptsd_files))

ptsd_files = glob.glob(os.path.join(os.path.join(src_folder, "PTSD"), "**", "*.mp4"), recursive=True)
ptsd_files = list(filter(lambda path: "copy" not in path.lower(), ptsd_files))

no_ptsd_files = random.sample(no_ptsd_files, k=len(ptsd_files))

paths = no_ptsd_files + ptsd_files

all_labels_ = [0] * len(no_ptsd_files) + [1] * len(ptsd_files)

paths = np.array(paths)
all_labels_ = np.array(all_labels_)

assert len(paths) == len(set(paths)), "there are duplicates"

text_paths = glob.glob("NO PTSD/**/*.txt", recursive=True) + glob.glob("PTSD/**/*.txt", recursive=True)
text_paths = list(filter(lambda path: "copy" not in path.lower(), text_paths))

create_dir_if_not_exists("split0")
create_dir_if_not_exists("split1")
create_dir_if_not_exists("split2")
create_dir_if_not_exists("text_split0")
create_dir_if_not_exists("text_split1")
create_dir_if_not_exists("text_split2")

kf = KFold(n_splits=3, shuffle=True, random_state=42)

for fold_num, (train_idx, test_idx) in enumerate(kf.split(paths)):
    print("Fold", fold_num)
    train_paths, train_labels = paths[train_idx], all_labels_[train_idx]
    test_paths, test_labels = paths[test_idx], all_labels_[test_idx]

    text_paths_train = []
    text_paths_test = []

    for text_path in text_paths:
        fname = os.path.basename(text_path).replace(".txt", "")
        if substr_is_in_list(fname, train_paths.tolist()):
            text_paths_train.append(text_path)
        elif substr_is_in_list(fname, test_paths.tolist()):
            text_paths_test.append(text_path)


    print("\tN_TRAIN_TEXT", len(text_paths_train))
    print("\tN_TEST_TEXT", len(text_paths_test))

    print("\tN_TRAIN_PTSD", (train_labels == 1).sum())
    print("\tN_TRAIN_NO_PTSD", (train_labels == 0).sum())
    print("\tN_test_PTSD", (test_labels == 1).sum())
    print("\tN_test_NO_PTSD", (test_labels == 0).sum())

    write_list_to_file(train_paths, f"split{fold_num}/train_paths.txt")
    write_list_to_file(test_paths, f"split{fold_num}/test_paths.txt")

    write_list_to_file(text_paths_train, f"text_split{fold_num}/train_paths.txt")
    write_list_to_file(text_paths_test, f"text_split{fold_num}/test_paths.txt")

"""## Do not run"""
#
# !zip -r split0.zip split0
# !zip -r split1.zip split1
# !zip -r split2.zip split2
# !zip -r text_split0.zip text_split0
# !zip -r text_split1.zip text_split1
# !zip -r text_split2.zip text_split2

"""# Moctar, imports"""

from dataclasses import dataclass
from torch.utils.data import Dataset, DataLoader, random_split
from transformers import BertTokenizer, BertModel, BertForSequenceClassification
import os
import glob
import torch
import torch.nn as nn
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score
from sklearn.model_selection import KFold, train_test_split
import random
import warnings
warnings.filterwarnings("ignore")
from transformers import logging
logging.set_verbosity_error()

"""# Moctar, Presets"""

device = "cuda:0" if torch.cuda.is_available() else "cpu"
print(f"Running on {device}")

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.random.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

"""# Moctar, classes"""

class PTSD_Text_Dataset(Dataset):
    def __init__(self, paths, labels, tokenizer):
        self.paths = paths
        self.labels = labels
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.paths)

    def __getitem__(self, idx):
        path = self.paths[idx]
        label = self.labels[idx]
        with open(path, "r") as f:
            text = f.read().lower()
        tokens = self.tokenizer(
            text,
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=512,
        )
        tokens["input_ids"] = tokens["input_ids"].squeeze()
        tokens["attention_mask"] = tokens["attention_mask"].squeeze()
        tokens["token_type_ids"] = tokens["token_type_ids"].squeeze()

        return tokens, label


class Model(nn.Module):
    def __init__(self, bert_model):
        super().__init__()
        self.bert_model = BertModel.from_pretrained("bert-base-uncased")
        self.dropout = nn.Dropout(0.1)
        self.fc = nn.Linear(768, 2)

    def forward(self, tokens):
        embeddings = self.bert_model(**tokens).last_hidden_state
        embeddings = embeddings.mean(dim=1)
        embeddings = self.dropout(embeddings)
        logits = self.fc(embeddings)
        return logits

"""# Moctar, train for single train-val-test split"""

torch.cuda.empty_cache()

train_paths_ptsd = glob.glob("train/PTSD/**/*.txt", recursive=True)
train_paths_no_ptsd = glob.glob("train/NO PTSD/**/*.txt", recursive=True)

validation_paths_ptsd = glob.glob("validation/PTSD/**/*.txt", recursive=True)
validation_paths_no_ptsd = glob.glob("validation/NO PTSD/**/*.txt", recursive=True)

test_paths_ptsd = glob.glob("test/PTSD/**/*.txt", recursive=True)
test_paths_no_ptsd = glob.glob("test/NO PTSD/**/*.txt", recursive=True)

print(
    f"\tN_train_PTSD: {len(train_paths_ptsd)}",
    f"\tN_train_NO_PTSD: {len(train_paths_no_ptsd)}",
)
print(
    f"\tN_validation_PTSD: {len(validation_paths_ptsd)}",
    f"\tN_validation_NO_PTSD: {len(validation_paths_no_ptsd)}",
)
print(
    f"\tN_test_PTSD: {len(test_paths_ptsd)}",
    f"\tN_test_NO_PTSD: {len(test_paths_no_ptsd)}",
)

train_paths = train_paths_ptsd + train_paths_no_ptsd
train_labels = [1] * len(train_paths_ptsd) + [0] * len(train_paths_no_ptsd)

validation_paths = validation_paths_ptsd + validation_paths_no_ptsd
validation_labels = [1] * len(validation_paths_ptsd) + [0] * len(validation_paths_no_ptsd)

test_paths = test_paths_ptsd + test_paths_no_ptsd
test_labels = [1] * len(test_paths_ptsd) + [0] * len(test_paths_no_ptsd)

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

train_dataset = PTSD_Text_Dataset(train_paths, train_labels, tokenizer)
val_dataset = PTSD_Text_Dataset(validation_paths, validation_labels, tokenizer)
test_dataset = PTSD_Text_Dataset(test_paths, test_labels, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=4)
test_loader = DataLoader(test_dataset, batch_size=4)

criterion = nn.CrossEntropyLoss().to(device)
bert_model = BertModel.from_pretrained("bert-base-uncased").to(device)
model = Model(bert_model).to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr = 5e-5, eps = 1e-08)

best_val_f1 = 0.0
for epoch in range(1, 6):
    print(f"\tEpoch: {epoch}/5")

    model.train()
    for i, batch in enumerate(train_loader):
        optimizer.zero_grad()
        tokens, labels = batch
        tokens = tokens.to(device)
        labels = labels.to(device)
        logits = model(tokens)
        loss = criterion(logits, labels)
        loss.backward()
        optimizer.step()

        print(f"\t\t{i+1}/{len(train_loader)} Train Loss: {loss.item()}")

    model.eval()
    eval_pred_list, eval_label_list = [], []
    for batch in val_loader:
        tokens, labels = batch
        tokens = tokens.to(device)
        labels = labels.to(device)
        logits = model(tokens)

        preds = torch.argmax(logits, dim=1).cpu().detach().numpy()
        labels_np = labels.cpu().detach().numpy()

        eval_pred_list.extend(preds.tolist())
        eval_label_list.extend(labels_np.tolist())

    val_f1 = f1_score(eval_label_list, eval_pred_list)
    print(f"\t\tValidation F1: {val_f1}")

    if val_f1 > best_val_f1:
        best_val_f1 = val_f1
        torch.save(model.state_dict(), "weights_best_val_f1.pt")

"""# Moctar, test for single train-val-test split"""

model.eval()
print(model.load_state_dict(torch.load("weights_best_val_f1.pt")))

test_pred_list = []
test_label_list = []
for batch in test_loader:
    tokens, labels = batch
    tokens = tokens.to(device)
    labels = labels.to(device)
    logits = model(tokens)

    preds = torch.argmax(logits, dim=1).cpu().detach().numpy()
    labels_np = labels.cpu().detach().numpy()

    test_pred_list.extend(preds.tolist())
    test_label_list.extend(labels_np.tolist())

test_f1 = f1_score(test_label_list, test_pred_list)
test_acc = accuracy_score(test_label_list, test_pred_list)
test_prec = precision_score(test_label_list, test_pred_list)
test_rec = recall_score(test_label_list, test_pred_list)

print(
    f"\tTesting Results For Single Split\n"
    f"\t\tAcc: {test_acc}\n"
    f"\t\tPrec: {test_prec}\n"
    f"\t\tRec: {test_rec}\n"
    f"\t\tF1: {test_f1}\n"
)

"""# Moctar, 3-Fold CV
## Important, change n_fold at line 10 to the one of [0,1,2]
"""

def read_list_from_file(path):
    l = []
    with open(path, "r") as f:
        return list(map(lambda p: p.replace("\n", ""), f.readlines()))



SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.random.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

torch.cuda.empty_cache()

# IMPORTANT
# Change n_fold to which split you are working on, one of [0,1,2]
n_fold = 2

train_paths = read_list_from_file(f"text_split{n_fold}/train_paths.txt")
test_paths = read_list_from_file(f"text_split{n_fold}/test_paths.txt")

train_paths_ptsd = list(filter(lambda p: p.split("/")[0] == "PTSD", train_paths))
train_paths_no_ptsd = list(filter(lambda p: p.split("/")[0] == "NO PTSD", train_paths))
test_paths_ptsd = list(filter(lambda p: p.split("/")[0] == "PTSD", test_paths))
test_paths_no_ptsd = list(filter(lambda p: p.split("/")[0] == "NO PTSD", test_paths))

print(f"FOLD: {n_fold+1}/3")
print(
    f"\tN_train_PTSD: {len(train_paths_ptsd)}",
    f"\tN_train_NO_PTSD: {len(train_paths_no_ptsd)}",
)
print(
    f"\tN_test_PTSD: {len(test_paths_ptsd)}",
    f"\tN_test_NO_PTSD: {len(test_paths_no_ptsd)}",
)

train_paths  = train_paths_ptsd + train_paths_no_ptsd
train_labels = [1] * len(train_paths_ptsd) + [0] * len(train_paths_no_ptsd)

test_paths   = test_paths_ptsd + test_paths_no_ptsd
test_labels  = [1] * len(test_paths_ptsd) + [0] * len(test_paths_no_ptsd)

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

train_dataset = PTSD_Text_Dataset(train_paths, train_labels, tokenizer)
test_dataset = PTSD_Text_Dataset(test_paths, test_labels, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=4)

criterion = nn.CrossEntropyLoss().to(device)
bert_model = BertModel.from_pretrained("bert-base-uncased").to(device)
model = Model(bert_model).to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr = 5e-5, eps = 1e-08)

for epoch in range(1, 6):
    print(f"\tEpoch: {epoch}/5")

    model.train()
    for i, batch in enumerate(train_loader):
        optimizer.zero_grad()
        tokens, labels = batch
        tokens = tokens.to(device)
        labels = labels.to(device)
        logits = model(tokens)
        loss = criterion(logits, labels)
        loss.backward()
        optimizer.step()

        print(f"\t\t{i+1}/{len(train_loader)} Train Loss: {loss.item()}")

torch.save(model.state_dict(), f"weights_split{n_fold}.pt")

"""# Moctar, test for nth fold"""

model.eval()
print(model.load_state_dict(torch.load(f"weights_split{n_fold}.pt")))

test_pred_list = []
test_label_list = []
for batch in test_loader:
    tokens, labels = batch
    tokens = tokens.to(device)
    labels = labels.to(device)
    logits = model(tokens)

    preds = torch.argmax(logits, dim=1).cpu().detach().numpy()
    labels_np = labels.cpu().detach().numpy()

    test_pred_list.extend(preds.tolist())
    test_label_list.extend(labels_np.tolist())

test_f1 = f1_score(test_label_list, test_pred_list)
test_acc = accuracy_score(test_label_list, test_pred_list)
test_prec = precision_score(test_label_list, test_pred_list)
test_rec = recall_score(test_label_list, test_pred_list)

print(
    f"\tTesting Results For fold = {n_fold}\n"
    f"\t\tAcc: {test_acc}\n"
    f"\t\tPrec: {test_prec}\n"
    f"\t\tRec: {test_rec}\n"
    f"\t\tF1: {test_f1}\n"
)
#
# Testing Results For fold = 0
# 		Acc: 0.9858490566037735
# 		Prec: 0.98
# 		Rec: 0.98989898989899
# 		F1: 0.9849246231155778
#
# Testing Results For fold = 1
# 		Acc: 0.9715639810426541
# 		Prec: 0.956140350877193
# 		Rec: 0.990909090909091
# 		F1: 0.9732142857142858
#
# Testing Results For fold = 2
# 		Acc: 0.9715639810426541
# 		Prec: 0.9722222222222222
# 		Rec: 0.9722222222222222
# 		F1: 0.9722222222222222
#
# (0.9858490566037735 + 0.9715639810426541 + 0.9715639810426541) / 3
#
# (0.98 + 0.956140350877193 + 0.9722222222222222) / 3
#
# (0.98989898989899 + 0.990909090909091 + 0.9722222222222222) / 3
#
# (0.9849246231155778 + 0.9732142857142858 + 0.9722222222222222) / 3

"""# Below is not important"""









import glob
import os
import random
from sklearn.model_selection import train_test_split
import shutil
import numpy as np
import time
from google.colab import drive
drive.mount("/content/drive", force_remount=True)

random.seed(42)
np.random.seed(42)

src_folder = "/content/drive/MyDrive/No   Yes PTSD videos"

no_ptsd_files = glob.glob(os.path.join(os.path.join(src_folder, "NO PTSD"), "**", "*.mp4"), recursive=True)
no_ptsd_files = list(filter(lambda path: "copy" not in path.lower(), no_ptsd_files))

ptsd_files = glob.glob(os.path.join(os.path.join(src_folder, "PTSD"), "**", "*.mp4"), recursive=True)
ptsd_files = list(filter(lambda path: "copy" not in path.lower(), ptsd_files))

no_ptsd_files = random.sample(no_ptsd_files, k=len(ptsd_files))

paths = no_ptsd_files + ptsd_files

assert len(paths) == len(set(paths)), "there are duplicates"

train_paths_ptsd, test_paths_ptsd = train_test_split(ptsd_files, test_size=0.2, random_state=42)
test_paths_ptsd, val_paths_ptsd = train_test_split(test_paths_ptsd, test_size=0.5, random_state=42)

train_paths_no_ptsd, test_paths_no_ptsd = train_test_split(no_ptsd_files, test_size=0.2, random_state=42)
test_paths_no_ptsd, val_paths_no_ptsd = train_test_split(test_paths_no_ptsd, test_size=0.5, random_state=42)

print("len(train_paths_ptsd)", len(train_paths_ptsd))
print("len(train_paths_no_ptsd)", len(train_paths_no_ptsd))

print("len(val_paths_ptsd)", len(val_paths_ptsd))
print("len(val_paths_no_ptsd)", len(val_paths_no_ptsd))

print("len(test_paths_ptsd)", len(test_paths_ptsd))
print("len(test_paths_no_ptsd)", len(test_paths_no_ptsd))

train_paths = train_paths_ptsd + train_paths_no_ptsd
val_paths = val_paths_ptsd + val_paths_no_ptsd
test_paths = test_paths_ptsd + test_paths_no_ptsd

print(train_paths)

print("====")
print("len(train_paths)", len(train_paths))
print("len(val_paths)", len(val_paths))
print("len(test_paths)", len(test_paths))

dest_folder = "drive/MyDrive/PTSD_Project_train_validation_test_split"

files_uploaded = glob.glob(dest_folder + "/**/*.mp4", recursive=True)
print(len(files_uploaded), len(set(files_uploaded)))
train_files_uploaded = glob.glob(dest_folder + "/train/**/*.mp4", recursive=True)
val_files_uploaded = glob.glob(dest_folder + "/validation/**/*.mp4", recursive=True)
test_files_uploaded = glob.glob(dest_folder + "/test/**/*.mp4", recursive=True)
# train_files_uploaded = [os.path.join(src_folder, "/".join(file.split("/")[4:])) for file in train_files_uploaded]
# val_files_uploaded = [os.path.join(src_folder, "/".join(file.split("/")[4:])) for file in val_files_uploaded]
# test_files_uploaded = [os.path.join(src_folder, "/".join(file.split("/")[4:])) for file in test_files_uploaded]

# diff_train = list(set(train_files_uploaded) - set(train_paths))
# diff_val = list(set(val_files_uploaded) - set(val_paths))
# diff_test = list(set(test_files_uploaded) - set(test_paths))
# print(len(train_paths), len(train_files_uploaded), len(diff_train), len(set(train_files_uploaded)), len(set(train_paths)))
# print(len(val_paths), len(val_files_uploaded), len(diff_val))
# print(len(test_paths), len(test_files_uploaded), len(diff_test))

# if not os.path.isdir(dest_folder):
#     os.makedirs(dest_folder)

# for i, src_path in enumerate(train_paths):
#     print(f"{i+1}/{len(train_paths)}")
#     drive.mount("/content/drive", force_remount=True)
#     new_path = "/".join(src_path.split("/")[5:])
#     dest_path = os.path.join(dest_folder, "train", new_path)
#     folder = os.path.split(dest_path)[0]
#     if not os.path.isdir(folder):
#         os.makedirs(folder)
#     shutil.copyfile(src_path, dest_path, follow_symlinks=False)
#     drive.flush_and_unmount()

# for i, src_path in enumerate(val_paths):
#     print(f"{i+1}/{len(val_paths)}")
#     drive.mount("/content/drive", force_remount=True)
#     new_path = "/".join(src_path.split("/")[5:])
#     dest_path = os.path.join(dest_folder, "validation", new_path)
#     folder = os.path.split(dest_path)[0]
#     if not os.path.isdir(folder):
#         os.makedirs(folder)
#     shutil.copyfile(src_path, dest_path, follow_symlinks=False)
#     drive.flush_and_unmount()

# for i, src_path in enumerate(test_paths):
#     print(f"{i+1}/{len(test_paths)}")
#     drive.mount("/content/drive", force_remount=True)
#     new_path = "/".join(src_path.split("/")[5:])
#     dest_path = os.path.join(dest_folder, "test", new_path)
#     folder = os.path.split(dest_path)[0]
#     if not os.path.isdir(folder):
#         os.makedirs(folder)
#     shutil.copyfile(src_path, dest_path, follow_symlinks=False)
#     drive.flush_and_unmount()

!zip -r PTSD_Project_train_validation_test_split.zip drive/MyDrive/PTSD_Project_train_validation_test_split

!zip -r /content/drive/MyDrive/zip/PTSD_Project_train_validation_test_split.zip drive/MyDrive/PTSD_Project_train_validation_test_split

drive.flush_and_unmount()











acc_arr = [0.9858490566037735, 0.995260663507109, 0.9715639810426541]
prec_arr = [0.98, 1.0, 0.9636363636363636]
rec_arr = [0.98989898989899, 0.990909090909091, 0.9814814814814815]
f1_arr = [0.9849246231155778, 0.995433789954338, 0.9724770642201834]

print("mean_acc", np.mean(acc_arr))
print("mean_prec", np.mean(prec_arr))
print("mean_rec", np.mean(rec_arr))
print("mean_f1", np.mean(f1_arr))







import glob
from os.path import join as join_paths
import os

def write_list_to_file(l, fname):
    with open(fname, "w") as f:
        for line in l:
            f.write(line)
            f.write("\n")

path = "/content/drive/MyDrive/PTSD_Project_train_validation_test_split"

no_ptsd = glob.glob(join_paths(path, "train", "NO PTSD", "**", "*.mp4"), recursive=True) \
        + glob.glob(join_paths(path, "validation", "NO PTSD", "**", "*.mp4"), recursive=True) \
        + glob.glob(join_paths(path, "test", "NO PTSD", "**", "*.mp4"), recursive=True)

ptsd = glob.glob(join_paths(path, "train", "PTSD", "**", "*.mp4"), recursive=True) \
     + glob.glob(join_paths(path, "validation", "PTSD", "**", "*.mp4"), recursive=True) \
     + glob.glob(join_paths(path, "test", "PTSD", "**", "*.mp4"), recursive=True)

no_ptsd = list(map(lambda p: os.path.basename(p), no_ptsd))
ptsd = list(map(lambda p: os.path.basename(p), ptsd))

write_list_to_file(no_ptsd, "no_ptsd_fnames.txt")
write_list_to_file(ptsd, "ptsd_fnames.txt")

